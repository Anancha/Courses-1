{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs) for Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.random.seed(1)\n",
    "ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Time Machine\" by H.G. Wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/time_machine.txt\", encoding='latin-1') as f:\n",
    "    time_machine = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Time Machine\n",
      "An Invention\n",
      "by H. G. Wells\n",
      "CONTENTS\n",
      "\n",
      "\n",
      " I Introduction\n",
      " II The Machine\n",
      " III The Time Traveller Returns\n",
      " IV Time Travelling\n",
      " V In the Golden Age\n",
      " VI The Sunset of Mankind\n",
      " VII A Sudden Shock\n",
      " VIII Explanation\n",
      " IX The Morlocks\n",
      " X When Night Came\n",
      " XI The Palace of Green Porcelain\n",
      " XII In the Darkness\n",
      " XIII The Trap of the White Sphinx\n",
      " XIV The Further Vision\n",
      " XV The Time Travellers Return\n",
      " XVI After the Story\n",
      " Epilogue\n",
      "\n",
      "\n",
      "\n",
      " I\n",
      "\n",
      "\n",
      " Introduction\n",
      "\n",
      "The Time Traveller (for so it will be c\n"
     ]
    }
   ],
   "source": [
    "print(time_machine[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing characters as numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n', 'W', 'd', 'X', '\\x94', 'B', 'h', 'ü', 'y', 'ç', 'o', 'K', 'b', 'g', 'M', '\\x91', '.', 'N', '?', 'C', 'e', 'a', 'r', 's', 'q', '\\x97', ']', 't', 'O', 'V', 'u', ')', 'm', 'H', 'F', 'w', '\\n', 'J', '\\x93', 'A', '-', ':', ';', '\\x9c', 'L', ' ', 'v', 'E', ',', 'P', '\\x85', 'f', 'D', 'l', 'S', 'x', '!', 'z', '(', 'T', 'i', '_', 'U', 'I', '[', 'c', 'Q', 'j', '\\x92', 'G', 'p', 'æ', 'R', 'k', 'Y']\n",
      "Length of vocab: 75\n"
     ]
    }
   ],
   "source": [
    "character_list = list(set(time_machine))\n",
    "vocab_size = len(character_list)\n",
    "print(character_list)\n",
    "print(\"Length of vocab: %s\" % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 0, 'W': 1, 'd': 2, 'X': 3, '\\x94': 4, 'B': 5, 'h': 6, 'ü': 7, 'y': 8, 'ç': 9, 'o': 10, 'K': 11, 'b': 12, 'g': 13, 'M': 14, '\\x91': 15, '.': 16, 'N': 17, '?': 18, 'C': 19, 'e': 20, 'a': 21, 'r': 22, 's': 23, 'q': 24, '\\x97': 25, ']': 26, 't': 27, 'O': 28, 'V': 29, 'u': 30, ')': 31, 'm': 32, 'H': 33, 'F': 34, 'w': 35, '\\n': 36, 'J': 37, '\\x93': 38, 'A': 39, '-': 40, ':': 41, ';': 42, '\\x9c': 43, 'L': 44, ' ': 45, 'v': 46, 'E': 47, ',': 48, 'P': 49, '\\x85': 50, 'f': 51, 'D': 52, 'l': 53, 'S': 54, 'x': 55, '!': 56, 'z': 57, '(': 58, 'T': 59, 'i': 60, '_': 61, 'U': 62, 'I': 63, '[': 64, 'c': 65, 'Q': 66, 'j': 67, '\\x92': 68, 'G': 69, 'p': 70, 'æ': 71, 'R': 72, 'k': 73, 'Y': 74}\n"
     ]
    }
   ],
   "source": [
    "# Creating a dictionary of the characters and their numerical representations\n",
    "character_dict = {}\n",
    "for e, char in enumerate(character_list):\n",
    "    character_dict[char] = e\n",
    "print(character_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation\n",
    "time_numerical = [character_dict[char] for char in time_machine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179683\n",
      "True\n",
      "[59, 6, 20, 45, 59, 60, 32, 20, 45, 14, 21, 65, 6, 60, 0, 20, 36, 39, 0, 45]\n",
      "The Time Machine\n",
      "An Invention\n",
      "by H. G. \n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "#  Check that the length is right\n",
    "#########################\n",
    "print(len(time_numerical))\n",
    "print(len(time_numerical) == len(time_machine))\n",
    "\n",
    "#########################\n",
    "#  Check that the format looks right\n",
    "#########################\n",
    "print(time_numerical[:20])\n",
    "\n",
    "#########################\n",
    "#  Convert back to text\n",
    "#########################\n",
    "print(\"\".join([character_list[idx] for idx in time_numerical[:39]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "def one_hots(numerical_list, vocab_size=vocab_size):\n",
    "    result = mx.nd.zeros((len(numerical_list), vocab_size), ctx=ctx)\n",
    "    for i, idx in enumerate(numerical_list):\n",
    "        result[i, idx] = 1.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]]\n",
      "<NDArray 3x75 @gpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(one_hots(time_numerical[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59, 6, 20]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_numerical[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_machine[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert it back\n",
    "def textify(embedding):\n",
    "    result = \"\"\n",
    "    indices = mx.nd.argmax(embedding, axis=1).asnumpy()\n",
    "    for idx in indices:\n",
    "        result += character_list[int(idx)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Time Machine\\nAn Invention\\nby H. G. Wells\\nCONTENTS\\n\\n\\n I Intro'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textify(one_hots(time_numerical[0:64]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Time Machine\\nAn Invention\\nby H. G. Wells\\nCONTENTS\\n\\n\\n I Intro'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining sequence length\n",
    "seq_length = 64\n",
    "# -1 here so we have enough characters for labels later\n",
    "num_samples = (len(time_numerical) - 1) // seq_length\n",
    "dataset = one_hots(time_numerical[:seq_length * num_samples]).reshape((num_samples, seq_length, vocab_size))\n",
    "textify(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sequences in dataset:  2807\n",
      "# of batches:  87\n",
      "Shape of data set:  (87, 64, 32, 75)\n",
      "87 batches of length 64, 32 examples in a batch, each having 75 one-hot encoded value for each character\n"
     ]
    }
   ],
   "source": [
    "print('# of sequences in dataset: ', len(dataset))\n",
    "num_batches = len(dataset) // batch_size\n",
    "print('# of batches: ', num_batches)\n",
    "train_data = dataset[:num_batches * batch_size].reshape((batch_size,\n",
    "                                                         num_batches,\n",
    "                                                         seq_length,\n",
    "                                                         vocab_size))\n",
    "# swap batch_size and seq_length axis to make later access easier\n",
    "train_data = mx.nd.swapaxes(train_data, 0, 1)\n",
    "train_data = mx.nd.swapaxes(train_data, 1, 2)\n",
    "print('Shape of data set: ', train_data.shape)\n",
    "print('87 batches of length 64, 32 examples in a batch, each having 75 one-hot encoded value for each character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Batch 0:***\n",
      " The Time Machine\n",
      "An Invention\n",
      "by H. G. Wells\n",
      "CONTENTS\n",
      "\n",
      "\n",
      " I Intro \n",
      " rely the\n",
      "mercury did not trace this line in any of the dimension \n",
      "\n",
      "\n",
      "***Batch 1:***\n",
      " duction\n",
      " II The Machine\n",
      " III The Time Traveller Returns\n",
      " IV Time \n",
      " s of Space\n",
      "generally recognised? But certainly it traced such a  \n",
      "\n",
      "\n",
      "***Batch 2:***\n",
      "  Travelling\n",
      " V In the Golden Age\n",
      " VI The Sunset of Mankind\n",
      " VII  \n",
      " line, and that\n",
      "line, therefore, we must conclude, was along the  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"***Batch %s:***\\n %s \\n %s \\n\\n\" % (i, textify(train_data[i, :, 0]), textify(train_data[i, :, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87, 64, 32, 75)\n"
     ]
    }
   ],
   "source": [
    "labels = one_hots(time_numerical[1:seq_length * num_samples + 1])\n",
    "train_label = labels.reshape((batch_size,\n",
    "                              num_batches,\n",
    "                              seq_length,\n",
    "                              vocab_size))\n",
    "train_label = mx.nd.swapaxes(train_label, 0, 1)\n",
    "train_label = mx.nd.swapaxes(train_label, 1, 2)\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etter look at it. Quartz it seemed\n",
      "to be.\n",
      "\n",
      "Look here, said the\n",
      "--------------------------------------------------\n",
      "tter look at it. Quartz it seemed\n",
      "to be.\n",
      "\n",
      "Look here, said the \n"
     ]
    }
   ],
   "source": [
    "print(textify(train_data[10, :, 3]))\n",
    "print('-' * 50)\n",
    "print(textify(train_label[10, :, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = vocab_size\n",
    "num_hidden = 64\n",
    "num_outputs = vocab_size\n",
    "\n",
    "########################\n",
    "#  Weights connecting the inputs to the hidden layer\n",
    "########################\n",
    "Wxh = mx.nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .01\n",
    "\n",
    "########################\n",
    "#  Recurrent weights connecting the hidden layer across time steps\n",
    "########################\n",
    "Whh = mx.nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx) * .01\n",
    "\n",
    "########################\n",
    "#  Bias vector for hidden layer\n",
    "########################\n",
    "bh = mx.nd.random_normal(shape=num_hidden, ctx=ctx) * .01\n",
    "\n",
    "########################\n",
    "# Weights to the output nodes\n",
    "########################\n",
    "Why = mx.nd.random_normal(shape=(num_hidden,num_outputs), ctx=ctx) * .01\n",
    "by = mx.nd.random_normal(shape=num_outputs, ctx=ctx) * .01\n",
    "\n",
    "# NOTE: to keep notation consistent,\n",
    "# we should really use capital letters\n",
    "# for hidden layers and outputs,\n",
    "# since we are doing batchwise computations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [Wxh, Whh, bh, Why, by]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y_linear, temperature=1.0):\n",
    "    lin = (y_linear - mx.nd.max(y_linear, axis=1)\\\n",
    "                    .reshape((-1, 1))) / temperature # shift each row of y_linear by its max\n",
    "    exp = mx.nd.exp(lin)\n",
    "    partition = mx.nd.sum(exp, axis=1).reshape((-1, 1))\n",
    "    return exp / partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.880797   0.11920292]\n",
       " [0.11920292 0.880797  ]]\n",
       "<NDArray 2x2 @cpu(0)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################\n",
    "# With a temperature of 1 (always 1 during training), we get back some set of probabilities\n",
    "####################\n",
    "softmax(mx.nd.array([[1, -1],\n",
    "                     [-1, 1]]),\n",
    "        temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.50049996 0.49949998]\n",
       " [0.49949998 0.50049996]]\n",
       "<NDArray 2x2 @cpu(0)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################\n",
    "# If we set a high temperature, we can get more entropic (*noisier*) probabilities\n",
    "####################\n",
    "softmax(mx.nd.array([[1,-1],\n",
    "                     [-1,1]]),\n",
    "        temperature=1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 0.]\n",
       " [0. 1.]]\n",
       "<NDArray 2x2 @cpu(0)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################\n",
    "# Often we want to sample with low temperatures to produce sharp probabilities\n",
    "####################\n",
    "softmax(mx.nd.array([[10,-10],\n",
    "                     [-10,10]]),\n",
    "        temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rnn(inputs, state, temperature=1.0):\n",
    "    outputs = []\n",
    "    h = state\n",
    "    for X in inputs:\n",
    "        h_linear = mx.nd.dot(X, Wxh) + mx.nd.dot(h, Whh) + bh\n",
    "        h = mx.nd.tanh(h_linear)\n",
    "        yhat_linear = mx.nd.dot(h, Why) + by\n",
    "        yhat = softmax(yhat_linear, temperature=temperature)\n",
    "        outputs.append(yhat)\n",
    "    return (outputs, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(yhat, y):\n",
    "    return - mx.nd.mean(mx.nd.sum(y * mx.nd.log(yhat),\n",
    "                                  axis=0,\n",
    "                                  exclude=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging the loss over the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_ce_loss(outputs, labels):\n",
    "    assert(len(outputs) == len(labels))\n",
    "    total_loss = 0.\n",
    "    for (output, label) in zip(outputs,labels):\n",
    "        total_loss = total_loss + cross_entropy(output, label)\n",
    "    return total_loss / len(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(prefix, num_chars, temperature=1.0):\n",
    "    #####################################\n",
    "    # Initialize the string that we'll return to the supplied prefix\n",
    "    #####################################\n",
    "    string = prefix\n",
    "\n",
    "    #####################################\n",
    "    # Prepare the prefix as a sequence of one-hots for ingestion by RNN\n",
    "    #####################################\n",
    "    prefix_numerical = [character_dict[char] for char in prefix]\n",
    "    input_v = one_hots(prefix_numerical)\n",
    "\n",
    "    #####################################\n",
    "    # Set the initial state of the hidden representation ($h_0$) to the zero vector\n",
    "    #####################################\n",
    "    sample_state = mx.nd.zeros(shape=(1, num_hidden), ctx=ctx)\n",
    "\n",
    "    #####################################\n",
    "    # For num_chars iterations,\n",
    "    #     1) feed in the current input\n",
    "    #     2) sample next character from from output distribution\n",
    "    #     3) add sampled character to the decoded string\n",
    "    #     4) prepare the sampled character as a one_hot (to be the next input)\n",
    "    #####################################\n",
    "    for i in range(num_chars):\n",
    "        outputs, sample_state = simple_rnn(input_v,\n",
    "                                           sample_state,\n",
    "                                           temperature=temperature)\n",
    "        choice = np.random.choice(vocab_size, p=outputs[-1][0].asnumpy())\n",
    "        string += character_list[choice]\n",
    "        input_v = one_hots([choice])\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipywidgets/widgets/widget.py:494: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  self.log.warn(message)\n",
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492a4696af7747dbb60c341bd790b255"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 199. Loss: 1.66059166545069\n",
      "The Time Machine and the strange and the same the strong the strong the started the strong the strong the strong the strong the strong the strong the strong the strong the strong the strong the streads of the strong the strong the strong the strong the strong the strong the strong the strong the strong the strong the strong the strange of the strange were was a seemed the strong the strong the strong the strong the strong the strong the strong the strange and the strong the strong the strong the strong the strong me to the little were were was a seemed and the strange were was a stared the strong the strange of the strengly and the sander and the strong the strong the strong the strong the strange of the strong the strong the strong and the strong the strong the strong the strong the strong the strong the strong the strange of the strong the strong the strong the strong me to the strong the strange of the strong the strong the strong the strong the strong the strong the strange of the strong the strong the strong the st\n",
      "The Medical Man rose, came to the lamp, and the strong the strange and the same and the strange and the strange and the strong the strange was a strong and the strong the strong the strong the strong the strong the strong the strange and the strong the strong the strong the strong the strong the strong the strong the started the same a mind of the strong the started the same a strong the strong the started the same and the strong the strong the strong the strong the strong the strong me to the strong the strong the strong the started the same and the same and she was the same the string the strong the strong the streads of the strong me to the little pround the strange and the strong the strong the strong the strange of the strange and the strong the strong the streads of the strange were were was the streads of the strange of the strong the strong the strong the strong the strong the strong the strong the streads of the strong the strong the strong the strong the strong the strong me to the little pround the same a seemed the strengly of the stra\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "moving_loss = 0.\n",
    "learning_rate = .5\n",
    "\n",
    "# state = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
    "for e in tqdm(range(epochs)):\n",
    "    ############################\n",
    "    # Attenuate the learning rate by a factor of 2 every 100 epochs.\n",
    "    ############################\n",
    "    if ((e+1) % 100 == 0):\n",
    "        learning_rate = learning_rate / 2.0\n",
    "    state = mx.nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
    "    for i in range(num_batches):\n",
    "        data_one_hot = train_data[i]\n",
    "        label_one_hot = train_label[i]\n",
    "        with mx.autograd.record():\n",
    "            outputs, state = simple_rnn(data_one_hot, state)\n",
    "            loss = average_ce_loss(outputs, label_one_hot)\n",
    "            loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "\n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        if (i == 0) and (e == 0):\n",
    "            moving_loss = np.mean(loss.asnumpy()[0])\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * np.mean(loss.asnumpy()[0])\n",
    "print(\"Epoch %s. Loss: %s\" % (e, moving_loss))\n",
    "print(sample(\"The Time Ma\", 1024, temperature=.1))\n",
    "print(sample(\"The Medical Man rose, came to the lamp,\", 1024, temperature=.1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
