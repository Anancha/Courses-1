{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Section 1 - Introduction and Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Notes\n",
    "- Reinforcement learning is more different from supervised and unsupervised machine learning are from each other.\n",
    "- Both supervised and unsupervised machine learning use gathered, labelled or unlabelled data.\n",
    "- Reinforcement learning interfaces with an environment (simulated environment or through sensors).\n",
    "- Objective of the RL is to minimize the cost / maximize the reward.\n",
    "- The agent makes actions and feedback signals (in form of the reward) are automatically given to the agent by the environment.\n",
    "- This course will only look at finite state space enviroment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tic-tac-toe example\n",
    "1) What is the number of states?\n",
    "- The board has 9 locations.\n",
    "- Each location on the board has 3 possibilities: empty, X and O.\n",
    "- The example is simplified but not finishing the game when a player wins. \n",
    "- With that the game has following number of states: $$ 3x3x3x3x3x3x3x3x3 = 3^9 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Important vocabulary\n",
    "- Agent: goal of the development, it interacts with the environment. Hopefully with an intelligent manner.\n",
    "- Environment: real or simulated world providing sensory data to the agent in for states, list of actions and a reward.\n",
    "- State: a configuration of the environment sensed by the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Notes\n",
    "- The agent tried to maximize its immediate reward, but future rewards as well\n",
    "- The reward must be programmer intelligently to avoid unintended consequences.\n",
    "- The reward as always a real number.\n",
    "- SAR = State, Action, Reward\n",
    "- Every game is a sequence of states, actions and rewards\n",
    "- Convention: start in state S(t), take action A(t), receive a reward of R(t+1)\n",
    "- R(t+1) is always a result of the agent making action A(t) as state S(t)\n",
    "- S(t) and A(t) result in environment changing to state S(t+1)\n",
    "- Triple [S(t), A(t), S(t+1)] is denoted as (s, a, s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Section 2 - Return of the Multi-Armed Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You go to a casino and choose between 3 slot machines.\n",
    "- Each returns a reward binary reward (0 or 1).\n",
    "- Win rate is unknown, it could be 0.1, 0.3, 0.5 respectively.\n",
    "- The goal is to maximize your winnings.\n",
    "- The best choice / tactic can only be discovered by collecting data.\n",
    "- It is important to balance the exploration (collecting the data) and exploit (abusing the best tactic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon Gready Strategy\n",
    "- A big advantage of this strategy is its simplicity.\n",
    "- Choose a small number epsilon as probability of exploration (typically 5% or 10%).\n",
    "- For each round we generate a random number.\n",
    "- If the number is less than epsilon then we pull a random arm.\n",
    "- If the number is bigger than epsilon then we pull currently the best arm.\n",
    "- In the long run, that theoretically allows us to explore each arm.\n",
    "- The problem of this approach is that with a given percentage, f.e. 10%, we spend 10% of the time exploring when it is no longer needed and thus choosing a suboptimal arm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Bandit Rewards\n",
    "- Bandit rewards assumed to be non-binaries (this approach works for a binary reward setup as well).\n",
    "- Reward tracking can be done with the mean value of the rewards of the previous episodes\n",
    "- Mean value is given as:\n",
    "$$\n",
    "\\bar{X} = \\frac{1}{N} \\sum_{i=1}^{N} X_{i}\n",
    "$$\n",
    "\n",
    "- This way of calculating the mean value is not scalabable, since we would need to store all the results from the previous episodes.\n",
    "- This problem can be solved as follows:\n",
    "$$\n",
    "\\bar{X}_{N} = \\frac{1}{N} \\sum_{i=1}^{N} X_{i} = \\frac{1}{N} \\sum_{i=1}^{N-1} X_{i} + \\frac{1}{N} X_{N} = \\frac{N-1}{N} \\bar{X}_{N-1} + \\frac{1}{N} X_{N}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bar{X}_{N} = (1 - \\frac{1}{N}) \\bar{X}_{N-1} + \\frac{1}{N} X_{N}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "#### Relevant files are: \n",
    "- 1-epsilon_greedy_strategy.py\n",
    "- 2_one-armed_bandit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimistic Initial Values\n",
    "- A simple approach to solving the explore-exploit dilemma is picking high initial value\n",
    "- We need to have some kind of knowledge about the process to do it\n",
    "- That means that while the data is collected the values for each of the arms are going to go down\n",
    "- All the values should finally converge to their true values, but since we are using the greedy algorithm, we ensure that arms that are not explored enough, will be chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "#### Relevant files are: \n",
    "- 3_one-armed_bandit_optimistic_init_values.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB1\n",
    "- Chernoff-Hoeffding bound states that the confidence bound changes exponentially with number of samples we collect:\n",
    "$$\n",
    "P\\left \\{ \\left | \\bar{X} - \\mu \\geq \\varepsilon  \\right | \\right \\} \\leq 2exp\\left \\{ -2\\varepsilon^{2}N \\right \\}\n",
    "$$\n",
    "\n",
    "- That leads to another, simpler equation:\n",
    "$$\n",
    "X_{UBC-j} = \\bar{X}_{j} + \\sqrt{2\\frac{lnN}{N_{j}}}\n",
    "$$\n",
    "where:\n",
    "    - N = number of times played in total\n",
    "    - Nj = number of times played bandit j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "#### Relevant files are: \n",
    "- 4-ucb1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
